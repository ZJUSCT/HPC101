{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('SC': conda)"
  },
  "interpreter": {
   "hash": "70365271746128711ef0b4014c5a603de93f49298d7f60b437c0a42596b47991"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 设置环境\n",
    "pytorch相比于tensorflow更容易入手，因此我们采用pytorch来完成下面的实验。\n",
    "\n",
    "推荐使用vscode进行实验，插件商店中中有jupyter notebook和tensorboard的插件"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置显卡环境，选择使用的显卡\n",
    "##torch.cuda.set_device(5)\n",
    "\n",
    "# 导入tensorboard，用来可视化训练情况\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter('./log/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "source": [
    "### 导入数据集\n",
    "获取MNIST数据集的方法有两种：\n",
    "\n",
    "1. 数字识别数据集MNIST在torchvision中有集成，可以参考 https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.MNIST 将其导入项目中\n",
    "\n",
    "2. 可从官网中 http://yann.lecun.com/exdb/mnist/ 下载到数据集，但需要注意数据集需要自己进行处理。以下是网站上给的数据存储方式：\n",
    "```\n",
    "TRAINING SET IMAGE FILE (train-images-idx3-ubyte):\n",
    "[offset] [type]          [value]          [description]\n",
    "0000     32 bit integer  0x00000803(2051) magic number\n",
    "0004     32 bit integer  60000            number of images\n",
    "0008     32 bit integer  28               number of rows\n",
    "0012     32 bit integer  28               number of columns\n",
    "0016     unsigned byte   ??               pixel\n",
    "0017     unsigned byte   ??               pixel\n",
    "........\n",
    "xxxx     unsigned byte   ??               pixel\n",
    "```\n",
    "最开始的四位被称为magic number，用于确认文件读取是否正确。其余内容参见description，一个pixel正好8个字节，范围为[0,255]。\n",
    "\n",
    "导入的数据需要变成Tensor以方便后面的操作，具体变换的格式可以参考 https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000 10000\n",
      "/home/taochenning/.conda/envs/SC/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "## 使用第一种方法读入data并将其转化成tensor，命名为MNIST_train和MNIST_test\n",
    "\n",
    "###############\n",
    "#此处加入你的代码#\n",
    "###############\n",
    "# trans = transforms.ToTensor()\n",
    "# MNIST_train = torchvision.datasets.MNIST(root = \"./Data\", train = True, transform = trans, download = False)\n",
    "# MNIST_test = torchvision.datasets.MNIST(root = \"./Data\", train = False, transform = trans, download = False)\n",
    "\n",
    "# 查看训练集和测试集的长度\n",
    "print(len(MNIST_train), len(MNIST_test))\n",
    "# 如果读入正确应该输出(60000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImg(MNIST_train, number):\n",
    "    \"\"\"显示具体的图像\"\"\"\n",
    "    (data, label) = MNIST_train[number]\n",
    "    img = np.squeeze(data)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(label, color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 251.565 263.63625\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-07-12T19:59:50.886695</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 263.63625 \nL 251.565 263.63625 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 239.758125 \nL 244.365 239.758125 \nL 244.365 22.318125 \nL 26.925 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p21ebd953fd)\">\n    <image height=\"218\" id=\"image286c9f6bc0\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-21.758125\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m7e41ecea99\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m7e41ecea99\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(27.626607 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m7e41ecea99\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(66.455179 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m7e41ecea99\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(102.1025 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m7e41ecea99\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m7e41ecea99\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(179.759643 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m7e41ecea99\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5e81b2bb91\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5e81b2bb91\" y=\"26.200982\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 30.000201)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5e81b2bb91\" y=\"65.029554\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 68.828772)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5e81b2bb91\" y=\"103.858125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 107.657344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5e81b2bb91\" y=\"142.686696\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 146.485915)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5e81b2bb91\" y=\"181.515268\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 185.314487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5e81b2bb91\" y=\"220.343839\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 224.143058)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 239.758125 \nL 26.925 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 239.758125 \nL 244.365 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 239.758125 \nL 244.365 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 22.318125 \nL 244.365 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_13\">\n    <!-- 3 -->\n    <g style=\"fill:#0000ff;\" transform=\"translate(131.8275 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-33\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p21ebd953fd\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# 修改数字可以看到不同的图像\n",
    "showImg(MNIST_train, 10)\n"
   ]
  },
  {
   "source": [
    "神经网络的训练一般会将网络分成若干个batch进行训练，我们使用pytorch的dataloader来对数据进行处理，详细文档参见 https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader\n",
    "注意使用后返回的是一个迭代器"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你可以修改不同的batch_size大小进行尝试\n",
    "batch_size = 100\n",
    "# 使用dataloader将数据加载进来\n",
    "#####################\n",
    "# 此处加入代码        #\n",
    "#####################\n",
    "# train_loader = DataLoader(dataset = MNIST_train, batch_size = 100, shuffle = True)\n",
    "# test_loader = DataLoader(dataset = MNIST_test, batch_size= 100, shuffle = True)"
   ]
  },
  {
   "source": [
    "### 网络结构\n",
    "使用pytorch定义网络结构十分方便，可以参考文档 https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=torch%20nn%20module#torch.nn.Module 进行书写。\n",
    "\n",
    "请你根据LeNet5论文中给出的神经网络结构完成一个Model class（详细内容请参考论文：http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf）\n",
    "\n",
    "![jupyter](./LeNet5.png)\n",
    "\n",
    "具体的连接层如何写也请参考文档 https://pytorch.org/docs/stable/nn.html\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self) :\n",
    "        super(Model, self).__init__()\n",
    "        ## 请在此加入你的网络定义\n",
    "        # self.conv1 = nn.Sequential(nn.Conv2d(1, 6, 3, 1, 1),\n",
    "        #                            nn.Sigmoid())\n",
    "        # self.avgPool1 = nn.Sequential(nn.AvgPool2d(2, 2))\n",
    "        # self.conv2 = nn.Sequential(nn.Conv2d(6, 16, 5),\n",
    "        #                            nn.Sigmoid())\n",
    "        # self.avgPool2 = nn.Sequential(nn.AvgPool2d(2, 2),\n",
    "        #                               nn.Flatten())\n",
    "        # self.linear1 = nn.Sequential(nn.Linear(16*5*5, 120),\n",
    "        #                              nn.Sigmoid())\n",
    "        # self.linear2 = nn.Sequential(nn.Linear(120, 84),\n",
    "        #                              nn.Sigmoid())\n",
    "        # self.linear3 = nn.Sequential(nn.Linear(84, 10),\n",
    "        #                              nn.GELU()) \n",
    "    def forward(self, x) :\n",
    "        ## 请在此加入前向传播函数\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.avgPool1(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.avgPool2(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.linear2(x)\n",
    "        # x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32543/2726117643.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 请检查神经网络每一步获得的tensor大小正确，检查的示例代码如下：\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m net = torch.nn.Sequential(torch.nn.Conv2d(1, 64, 5, 5, 5),\n\u001b[0m\u001b[1;32m      3\u001b[0m                           torch.nn.ReLU())\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 随机生成一个输入向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 输入向量是(1, 1, 28, 28)的原因：一般一个batch的tensor的形状是(B, C, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# 请检查神经网络每一步获得的tensor大小正确，检查的示例代码如下：\n",
    "net = torch.nn.Sequential(torch.nn.Conv2d(1, 64, 5, 5, 5),\n",
    "                          torch.nn.ReLU())\n",
    "# 随机生成一个输入向量\n",
    "# 输入向量是(1, 1, 28, 28)的原因：一般一个batch的tensor的形状是(B, C, H, W)\n",
    "# B为一个batch中图片的数量\n",
    "# C为通道数量\n",
    "# H为高\n",
    "# W为宽\n",
    "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape: \\t', X.shape)"
   ]
  },
  {
   "source": [
    "### 损失函数与优化方法\n",
    "下面的代码中你需要做三件事情：\n",
    "\n",
    "1. 确定模型运行的位置（若在gpu中运行需要将模型移动到gpu上）\n",
    "\n",
    "2. 定义损失函数 (https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "\n",
    "3. 选择优化器 (https://pytorch.org/docs/stable/optim.html?highlight=torch%20optim#module-torch.optim)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model().to(device)\n",
    "# cost = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "source": [
    "### 训练\n",
    "提示：\n",
    "\n",
    "1. 利用backward()进行方向传播的梯度计算，optimizer.step()进行梯度下降\n",
    "\n",
    "2. 注意数据存放的位置\n",
    "\n",
    "若成功可以在tensorboard中看到训练loss和准确度的变化\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "model.train()\n",
    "with SummaryWriter() as writer:\n",
    "    for epoch in range(epochs):\n",
    "        sum_loss = 0.0\n",
    "        train_correct = 0 # 用来统计判断正确的数量\n",
    "        ## 加入你的代码\n",
    "        # for i, data in enumerate(train_loader):\n",
    "        #     inputs, lables = data\n",
    "        #     inputs, lables = inputs.to(device), lables.to(device)\n",
    "        #     optimizer.zero_grad()\n",
    "        #     outputs = model(inputs)\n",
    "        #     loss = cost(outputs, lables)\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "\n",
    "        #     _, id = torch.max(outputs.data, 1)\n",
    "        #     sum_loss += loss\n",
    "        #     train_correct += torch.sum(id == lables.data)\n",
    "\n",
    "        writer.add_scalar('loss', sum_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar('train_acc', 100 * train_correct / len(MNIST_train), epoch)"
   ]
  },
  {
   "source": [
    "### 测试\n",
    "步骤和训练类似，需要得到正确的个数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "correct: 86.390%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# 用来统计正确的数量\n",
    "test_correct = 0\n",
    "\n",
    "## 插入你的代码\n",
    "# for i, data in enumerate(test_loader):\n",
    "#     inputs, lables = data\n",
    "#     inputs, lables = inputs.to(device), lables.to(device)\n",
    "#     outputs = model(inputs)\n",
    "#     _, id = torch.max(outputs.data, 1)\n",
    "#     test_correct += torch.sum(id == lables.data)\n",
    "\n",
    "print(\"correct: %.3f%%\" % (100 * test_correct / len(MNIST_test)))"
   ]
  },
  {
   "source": [
    "### 思考问题：\n",
    "1. 池化层有很多种，较常使用的是平均和最大，在这个数据集上使用有什么的区别，哪个效果好一些？\n",
    "2. 修改网络结构以求得到更好的效果，可能的修改方向：\n",
    "\n",
    "    a. 调整卷积窗口的大小、步长\n",
    "\n",
    "    b. 调整卷积层的数量\n",
    "\n",
    "    c. 调整全连接层的数量\n",
    "\n",
    "    d. 调整激活函数\n",
    "    \n",
    "    e. 调整输出通道的多少\n",
    "3. 在MNIST数据集上尝试改进网络\n",
    "4. 可以用tensorboard来可视化中间输出（可以清晰得看到不同池化层的效果）\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 6, 28, 28])\ntorch.Size([6, 1, 28, 28])\ntorch.Size([1, 6, 14, 14])\ntorch.Size([6, 1, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# 使用之前训练好的网络\n",
    "from torchvision import utils\n",
    "class Visualize(nn.Module):\n",
    "    def __init__(self) :\n",
    "        super(Visualize, self).__init__()\n",
    "    def forward(self, input_img):\n",
    "        writer = SummaryWriter()  \n",
    "        for i, layer in enumerate(model.children()):\n",
    "            print(layer)\n",
    "            input_img = layer(input_img)\n",
    "            # 前面2层输出，后面是全连接层，因此没法可视化\n",
    "            if i <= 1:\n",
    "                # 进行你的可视化操作\n",
    "                # 参考文档：\n",
    "                # tensorboardX: https://tensorboardx.readthedocs.io/en/latest/tutorial.html#add-image\n",
    "                ##### 插入你的代码 ######\n",
    "                # # 第0和第1维转置，交换位置(为了适应make_grid函数)\n",
    "                # input_img = input_img.permute(1, 0, 2 ,3)       \n",
    "                # # 一个channel代表一张图片，把每层的多个图片拼接成一张大图\n",
    "                # result = torchvision.utils.make_grid(input_img, normalize=True)  \n",
    "                # writer.add_image(str(i), result, i)\n",
    "                # # 将纬度变回正常\n",
    "                # input_img = input_img.transpose(1, 0) \n",
    "        writer.close()\n",
    "        # permute(1,0,2,3)需要把所有维度都写出来；transpose(1,0)可以只把需要转置的几个维度写出来；功能是差不多的\n",
    "\n",
    "# 表示取出第1个batch中的第4张图片\n",
    "batch_number = 1\n",
    "data_number = 4\n",
    "\n",
    "for i, test in enumerate(test_loader):\n",
    "    if i == batch_number:\n",
    "        data, lable = test\n",
    "\n",
    "newModel = Visualize().to(device)\n",
    "image = data[data_number].to(device)\n",
    "# 修正输入向量，使其满足格式\n",
    "image = image.unsqueeze(0)\n",
    "newModel(image) \n",
    "# 完成后可在tensorboardX中查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}